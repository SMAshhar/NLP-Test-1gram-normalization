{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  1709k      0  0:00:48  0:00:48 --:--:-- 3568k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "base_dir = pathlib.Path(\"aclImdb\")              # defining base directory\n",
    "val_dir = base_dir / \"val\"                      # defining validation directory\n",
    "train_dir = base_dir / \"train\"                  # defining train directory\n",
    "for category in (\"neg\", \"pos\"):                 \n",
    "    os.makedirs(val_dir / category)             \n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname, val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 21:36:51.557950: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination\n",
      "2022-10-18 21:36:51.558041: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: v2fftb-HP-Pavilion-Gaming-Laptop-15-dk1xxx\n",
      "2022-10-18 21:36:51.558050: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: v2fftb-HP-Pavilion-Gaming-Laptop-15-dk1xxx\n",
      "2022-10-18 21:36:51.558265: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.85.2\n",
      "2022-10-18 21:36:51.558305: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.65.1\n",
      "2022-10-18 21:36:51.558317: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 515.65.1 does not match DSO version 510.85.2 -- cannot find working devices in this configuration\n",
      "2022-10-18 21:36:51.560038: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size = batch_size\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size = batch_size\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size = batch_size\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape is :  (32,)\n",
      "Input's dataype is :  <dtype: 'string'>\n",
      "Target's shape is :  (32,)\n",
      "Target's datatype is :  <dtype: 'int32'>\n",
      "Input[0] :  tf.Tensor(b\"An unpleasant woman and an equally unpleasant man are violently and horribly assaulted by a group of two-dimensional psycho thugs during a night-time encounter on a forest road in Shropshire, England. The man and woman who were assaulted plan and carry out a revenge attack on their attackers...<br /><br />Utterly repellent piece of voyeuristic trash, somehow masquerading as 'thought-provoking' drama, whilst actually coming across as sub-Michael Winner cr*p (you just know that Oliver Reed and Susan George would have been cast had it so easily have been made in the 1970s). What happens to Alice (Gillian Anderson) and Adam (Danny Dyer) is appalling and devastating, yet Dan Reed somehow manages to rub the viewer's nose in every last glob of its sexual nastiness. His camera lingers hungrily on Anderson's naked body both during and after the assault, whilst the script leaves almost all the characters floundering in a turgid sea of two dimensional clich\\xc3\\xa9. His script forces his characters to behave in such a way as to alienate the viewer further from the 'victims' by shoving more ghastly situations into their faces (Adams's attempted post-incident assaults on both Sophie and Alice; Alice's assault on Heffer AFTER his suicide-attempt confession).<br /><br />The quandary comes from the central protagonists' performances - Dyer is a horrible actor, incapable of light and shade as the young male victim of the initial assault (he'll end up in Eastenders, mark my words), but Anderson is extraordinary. Even as the atrocious script forces her character to behave in depraved and ludicrous ways, she somehow delivers an extraordinarily compelling and complicated characterisation as a self-indulgent, arrogant hedonist who encounters such horrors and needs to retaliate.<br /><br />A vile and pointless film then, almost but not quite rescued by a compelling central female performance.\", shape=(), dtype=string)\n",
      "Targets[0] :  tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"Input shape is : \", inputs.shape)\n",
    "    print(\"Input's dataype is : \", inputs.dtype)\n",
    "    print(\"Target's shape is : \", targets.shape)\n",
    "    print(\"Target's datatype is : \", targets.dtype)\n",
    "    print(\"Input[0] : \", inputs[0])\n",
    "    print(\"Targets[0] : \", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing words as a set: The bag-of-words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,               # Limiting max tokens to 20000\n",
    "    output_mode=\"multi_hot\",        # Keeping output mode to be multihot binary vector\n",
    ")\n",
    "\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)        # Taxt extracted from file (label plus text)\n",
    "text_vectorization.adapt(text_only_train_ds)             # Adapting text vactorization \n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens = 20000, hidden_dim = 16):\n",
    "    inputs = keras.Input(shape = (max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation ='relu')(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                    loss=\"binary_crossentropy\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 19s 28ms/step - loss: 0.3991 - accuracy: 0.8314 - val_loss: 0.2864 - val_accuracy: 0.8946\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.2645 - accuracy: 0.9022 - val_loss: 0.2795 - val_accuracy: 0.8960\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.2381 - accuracy: 0.9173 - val_loss: 0.2929 - val_accuracy: 0.8944\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.2236 - accuracy: 0.9250 - val_loss: 0.3101 - val_accuracy: 0.8966\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.2098 - accuracy: 0.9316 - val_loss: 0.3205 - val_accuracy: 0.8956\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.2149 - accuracy: 0.9324 - val_loss: 0.3320 - val_accuracy: 0.8916\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 14s 22ms/step - loss: 0.2109 - accuracy: 0.9330 - val_loss: 0.3422 - val_accuracy: 0.8882\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.2090 - accuracy: 0.9342 - val_loss: 0.3462 - val_accuracy: 0.8900\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.2072 - accuracy: 0.9378 - val_loss: 0.3558 - val_accuracy: 0.8896\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.1994 - accuracy: 0.9395 - val_loss: 0.3602 - val_accuracy: 0.8912\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.2862 - accuracy: 0.8893\n",
      "Test acc: 0.889\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
    "        save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "        validation_data=binary_1gram_val_ds.cache(),\n",
    "        epochs=10,\n",
    "        callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = \"\"\"Like a scene in a movie or a verse in a song, paragraphs are the building blocks of any good piece of writing. Paragraphs lend a natural rhythm to your writing that makes it a joy to read. The question is, how do you handle them successfully? \n",
    "Below, we take a close look at what makes up an effective paragraph and explain how to write one that suits your needs. We also cover some advanced tips. But first, letâ€™s start with the fundamentals. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.06 percent positive\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "outputs = model(processed_inputs)\n",
    "inference_model = keras.Model(inputs, outputs)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "raw_text_data = tf.convert_to_tensor([[\"This is not a good movie. But for senior citizens would like it. New gen will hate it\"],])\n",
    "predictions = inference_model(raw_text_data)\n",
    "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
